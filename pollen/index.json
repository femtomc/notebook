{"notes/aide.html":{"body":"AIDE: towards automatic inference tuninghttps://arxiv.org/pdf/1705.07224.pdfAIDE is a technique for measuring the accuracy of probabilistic inference algorithms by computing an estimator for the symmetric Kullback-Leibler (KL) divergence between the approximating distributions of two inference algorithms.In this note, I’ll be working through the paper - attempting to provide informal context where necessary.BackgroundFirst, some background. In approximate Bayesian inference, the goal is to compute an approximation of the posterior that converges to the true posterior in some limit of computation. In Monte Carlo (importance) sampling, for example, the limit may be the number of samples. In Markov chain Monte Carlo (MCMC), this limit is the number of transition steps.Often times, these limits are framed by considering an expectation taken with respect to the target distribution, e.g.\\begin{equation}E[X;P] \\approx \\hat{E}_n[X;P] = \\frac{1}{n}\\sum_{i=1}^n x_i\\end{equation}These algorithms are configurable.In importance sampling, the user can provide a proposal distribution which the algorithm will sample from (but the proposal must cover the support of the posterior). Intuitively, the closer the proposal is to the true posterior, the smaller the number of samples required to reduce the variance to a specified tolerance. So the choice of proposal distribution has ramifications for the amount of computation required to estimate the expectation. The theoretical “best” proposal distribution requires only a single sample - in practice, usage almost always necessitates taking a number of samples and computating the expectation using a weighted average. A good reference here on proposals, variance bounds, and computation is https://projecteuclid.org/journals/annals-of-applied-probability/volume-28/issue-2/The-sample-size-required-in-importance-sampling/10.1214/17-AAP1326.fullChatterjee and Diaconis, 2018.A similar characterization is true for MCMC, where the user can provide a kernel (which must satisfy a property called detailed balance with respect to the target distribution to ensure that the Markov chain targets the correct distribution). The computational limit parameter in question is the length of the Markov chain.Often times, categorization of these algorithms discuss these explicit parameters in great detail - but in practice, there are also implicit computational costs which must be considered when designing custom inference algorithms. In importance sampling, sampling from the proposal distribution and evaluation of log-likelihood functions are two computational processes which are effectively involved in every sampling step. In MCMC, sampling from the kernel and evaluation of the accept-reject ratio are involved in every step.These implicit costs must be considered in the practical design of inference - e.g. an algorithm which theoretically converges quickly may practically be unacceptable under computational restrictions. In many practical cases of Bayesian inference, effective algorithms which are known to be empirically successful from the literature will be run for a restricted number of steps. In MCMC, for example, this induces accuracy penalties against the true posterior. So the goal, really, is not choosing an algorithm by comparing behavior in the theoretical $n \\rightarrow \\infty$ limit, but comparing algorithms within a fixed computational restriction.AIDE is a technique which seeks to address part of this design problem.assets/aide.pngThe AIDE architecture, as presented in https://arxiv.org/pdf/1705.07224.pdfCusumano-Towner and Mansinghka, 2017.Inference and meta-inferenceOne important thing to understand is that AIDE sits within a conceptual framework for probabilistic and inference programming developed by (https://www.mct.dev/Marco Cusumano-Towner http://probcomp.csail.mit.edu/et al) called https://www.gen.dev/Gen. The paper is full of terminology which overlaps with this project - including things like traces and generative functions.In particular, the way that AIDE is applied to inference algorithms is by framing them as generative inference models - https://www.mct.dev/assets/mct-thesis.pdfgenerative models (whose mathematical background is covered in great depth in Marco’s thesis) which return the output density of the inference algorithm when marginalized over internal random choices. This choice essentially motivates what follows - the difficulty of computing densities of inference algorithms occurs because it is difficult to marginalize over internal random choices of the algorithm. The authors specify: let’s treat inference algorithms as generative models, and then use techniques from marginal likelihood estimation in a Monte Carlo estimator of divergence between output distributions of the two algorithms.Generative inference modelA tuple $(U, X, q)$ where $q(u, x)$ is a joint density on $U \\times X$. A generative inference model models an inference algorithm if the output density of the inference algorithm is the marginal $q(x) = \\int q(u, x) \\ du$. An element $u \\in U$ represents a complete assignment to the internal random choices within the inference algorithm (and, following Gen terminology, is called a trace). The interface to generative inference models require that it is possible to simulate (sample) from $q(u, x)$ but analytic computation of the density is not required.Meta-inference algorithmFor a given generative inference model $(U, X, q)$, a meta-inference algorithm is a tuple $(r, \\xi)$ where $r(u; x)$ is a density on traces $u \\in U$ indexed (possibly implicitly conditioned on, or represented as a function which accepts) on $x$. $\\xi(u, x)$ is defined as\\begin{equation}\\xi(u, x) = Z \\frac{q(u, x)}{r(u; x)}, \\text{for some $Z > 0$}\\end{equation}The interface to meta-inference algorithms requires the ability to sample $u ~ r(u; x)$ and evaluate $\\xi(u, x)$ for specified $u$ and $x$.(https://arxiv.org/pdf/1705.07224.pdfc.f. section 3.1 of the paper) Conceptually, a meta-inference sampler tries to answer the question “how could my inference algorithm have produced this output $x$?”Note that if it is tractable to evaluate the marginal likelihood $q(x)$ of the generative inference model up to a normalizing constant, then it is not necessary to represent internal random variables for the inference algorithm, and a generative inference model can define the trace as an empty token $u = ()$ with $U = \\{()\\}$. In this case, the meta-inference algorithm has $r(u; x) = 1, \\forall x$ and $ξ(u, x) = Zq(x)$.So, what exactly does a generative inference model and meta-inference algorithm look like?The authors present a meta-inference algorithm for SMC (which I discuss a bit more in xlinknotes/unreasonable_smc.htmlxlink../notes/unreasonable_smc.htmlThe unreasonable effectiveness of rejuvenation move SMC and xlinknotes/smc_samplers.htmlxlink../notes/smc_samplers.htmlNotes on sequential Monte Carlo).assets/aide_smc.pngMeta-inference sampler for SMC, as presented in https://arxiv.org/pdf/1705.07224.pdfCusumano-Towner and Mansinghka, 2017.The estimatorA key part of AIDE is the usage of the symmetrized KL divergence as a measure of difference between distributions. The KL divergence is defined as\\begin{equation}D_{KL}(P, Q) = \\sum_{x\\in X}P(x) \\log \\big( \\frac{P(x)}{Q(x)} \\big)\\end{equation}which is not a metric, although it satisfies some properties of metrics, like $D_{KL}(P, Q) \\geq 0$ with saturation if $P \\equiv Q$ almost everywhere. The symmetrized KL divergence looks like the following\\begin{equation}D_{KL}(P, Q) = \\sum_{x\\in X}(P(x) - Q(x)) \\log \\big( \\frac{P(x)}{Q(x)} \\big)\\end{equation}assets/aide_alg.pngThe AIDE algorithm, as presented in https://arxiv.org/pdf/1705.07224.pdfCusumano-Towner and Mansinghka, 2017.","id":"notes/aide.html","title":"AIDE: towards automatic inference tuning"},"notes/compiler_desires.html":{"body":"Feelings about language and compiler implementationsThis is going to be a bit of a ramble. I feel like I complain about language and compiler design features more than your average programmer. I think it’s perfectly fine to complain about things like this, especially in programming - where there are strong incentives (even tendencies!) to settle into ecosystem-based language optima. I suspect the study and usage of programming languages exemplifies the “perfect is the enemy of the good” ethos. Nonetheless, new ideas which radically simplify things will always be attractive to me - even if the cost in a language implementation is that you have to start with a young ecosystem.I know what I like in programming languages. I started with Python,  then I used Julia, then I messed around with Rust, then I used C for a bit, which helped me realize that Rust was really nice, then I used Julia a lot, then I played around with Haskell. Along the way, I’ve used Scala, OCaml, Racket, Zig, Coq, even a little bit of VHDL for an FPGA course project. By far, I’m most comfortable with Julia and Rust. Of course, I reach for them for different things - but I’m reasonably confident that I could put together medium to large codebases in each language and not have it be a total disaster.I like Julia for many reasons. It feels like the “code is data, data is code” ethos of Lisp has been built into the DNA of the compiler. The official language compiler implementation is also fast, and the community is obsessed with making it faster in the ways that count. The community is also quite smart–I’m learned more than a measurable amount of useful tidbits just by engaging with the community.The language implementation is also well-designed, even though it currently requires a GC - the compiler is smart enough to not generate a lot of garbage. The language combination of subtyping and multiple dispatch has created this composability solace (from the framework chaos which dominates modern computing). Julia was sort of like my gateway drug to language/compiler things. On the other hand, there are things about Julia which frustrate me. When you start to get into the Julia compiler, there are packages which lull you into a false sense of promise - the promise that you (yes, you, intrepid programmer) can explore truly insane things like full language AD, or plugging compiler passes into the hot running runtime, even changing inference (which is for performance, not correctness, so it’s all good). In practice, there are limitations built into the compiler (for good reasons! which make sense when you start to read about Julia’s type inference, and the requirements of dynamism). There are pedantic things, like the “unorthodox” (at least, compared to traditional AOT compilers) workflow for compiling Julia code to a binary. Or the fact that Julia’s AD story is sort of ... in a rut (at least, at the time of this writing, early 2022). Most of all, I think I’ve used and enjoyed Julia more than any other language I’ve played around with - and sometimes I’ve just wanted to expand my horizons, explore other ideas, etc. Julia is beautiful, but no matter where you are at, sometimes you might get the “grass is always greener” sort of feeling.I like Rust because the compiler toolchain is (I think, bar none) the best thing I’ve ever used. It’s crazy how many QoL things are just part of the toolchain, and just work seamlessly. Using the Rust compiler honestly feels like cheating. There’s this weird quality to the Rust workflow which I can only encapsulate with “solid” or “robust”. It just feels good. Coming from Julia (with Julia’s emphasis on multiple dispatch and subtyping), I appreciate traits - sometimes I’ve reached for idioms which I would use in Julia, and I forget that subtyping is not a thing in Rust (although dynamic dispatch via trait objects is a thing). The error handling is also very well done (especially compared to Go, for example, or other languages which don’t even provide special syntax or facilities for propagating errors).Things I dislikeI sort of hate Python - but not really enough to work myself into a passion about it. I just sort of think it’s bland. It’s a bland language, the native interpreter-based implementation is slow, lack of any form of typing hurts my soul and prevents me from rapidly reading Python code (I think this is the big thing for me). “Bind the real framework from C++ in Python” things appear to be like composability poison (especially if you get used to Julia, where everything “just composes” via multiple dispatch and subtyping).I hate C++, probably with an intensity which is unwarranted. I just fundamentally do not understand (outside of ecosystem momentum! which is obviously a big thing) why anyone would use C++ today. The tooling feels like complicated garbage (conan seems alright, but then that’s a project-specific thing–and not enforced like Rust “strong opinion” enforces cargo), CMake feels complicated, imposing, just not worth it? Templates are too powerful, and lead to incredibly complicated debugging. Even the very, very small amount of work with templates I’ve done has scared me. Metaprogramming templates–which seems to increasingly be a modern accepted idiom, honestly nauseates me. It’s one of these things where it feels like either I’m going to have to spend 6 months to a year to learn it deeply, or I’m just not going to get it. That’s a waste of time in my opinion–just to learn the tool! It’s not even the things I want to express, it’s just the stupid tool I need to use to express them.In my biased head, Rust is basically superior to C++ in every single way except for ecosystem, which is one of the most important reasons for why anyone uses any language. I feel sort of comfortable with Rust, I would consider myself an intermediate Rust programmer. I’ve spent many weeks looking at C++ code, trying to get certain compiler things working (LLVM/MLIR) with the CMake tooling - I’ve just found it be hard, in a cognitively draining way.","id":"notes/compiler_desires.html","title":"Feelings about language and compiler implementations"},"notes/cpssa.html":{"body":"Some thoughts about CPS & SSAConsider a simple block IR:highlightcodeexcludehaskell1: (%1, %2)  {\n    %3 = intrinsic_add(%1, %2)\n    %4 = intrinsic_add(%3, %3)\n    return %4\n}where codeexcludeintrinsic_add is an intrinsic for our IR language - e.g. it can be mapped directly onto an assembly element, and doesn’t require allocating a stack frame (for example).In our basic block here, there is no flow of control out of the block until the computation terminates. In a functional language like Haskell, you could image replacing codeexcludereturn with an effectful operator (e.g. something which requires a monadic context) that interacts with the outside world. In other words, the completion of the computation performs some effectful action (so we see the result in codeexclude%4) and then no other flow of control is performed.What if our function calls another function which is not a primitive?highlightcodeexcludehaskell1: (%1, %2)  {\n    %3 = non_primitive(%1, %2)\n    %4 = intrinsic_add(%3, %3)\n    return %4\n}In SSA, we have to represent the transfer of control to the codeexcludenon_primitive (which will be represented by basic blocks). But now, when the codeexcludenon_primitive call returns, where does it transfer control to? To handle this, the real block representation would actually look like this:highlightcodeexcludehaskell-- I'm abusing SSA here by re-using register names.\n1: (%1, %2)  {\n    invoke 2 (%1, %2)\n}\n\n2: (%1, %2) {\n    %3 = some_other_intrinsic(%1, %2)\n    invoke 3 (%3)\n}\n\n3: (%1) {\n    %2 = intrinsic_add(%1, %1)\n    return %2\n}Now, the flow of control is explicit, and linear - we don’t have to “jump to the middle” of a block. Furthermore, every block knows how to transfer control to a target. Notice how this optimizes manipulation of stack frames - each transfer of control indicates a complete de-allocation (e.g. no required saving/restoring). This is exactly equivalent to tail call optimization.But this is exactly the same idiom as CPS - when we write:highlightcodeexcludehaskellf x k c = c $ some_computation x keach continuation codeexcludec also knows where to jump to next, and never has a return back.Let’s also look at recursion in the block representation by studying codeexcludefactorial:highlightcodeexcludehaskellfunction factorial(n)\n    if (n == 0)\n        return 1\n    else\n        return n * factorial(n - 1)\n    end\nendThis might (pseudo) lower to:highlightcodeexcludehaskell1: (%1) {\n    %2 = primitive_check(%1, 0)\n    check %2 then invoke 2 (1) -- problematic.\n    %3 = primitive_sub(%1, 1)\n    %4 = invoke 1 (%3)\n    %5 = primitive_mul(%1, %4)\n    invoke 2 (%5)\n}\n\n2: (%1) {\n    return %1\n}But notice how we’ve broken the nice property discussed above - now, we “return to the middle of a block” which means we have to do some costly stack frame manipulation. How do we handle this in CPS?highlightcodeexcludehaskellfact_cps n c =\n    if n == 0 then c 1\n              else fact_cps (n - 1) \\v -> c (n * v)\n                                    ---------------\n                                    --  closure  --When codeexcluden == 0, we follow the branch by invoking the continuation codeexcludec explicitly on codeexclude1 - otherwise, we accumulate the result in the continuation which we pass to the recursive call.We can take inspiration and re-visit the block representation. There’s a few issues - one is that our functional language supports closures, and I haven’t specified how this works in the block language. No problem - we’ll use https://en.wikipedia.org/wiki/Lambda_liftinglambda lifting and imagine giving each closure a unique name in global scope. This means that the closure will be converted into a block with (ignoring the closed-over continuation codeexcludec for the moment) 2 arguments.Now, the closure closes over the continuation codeexcludec - this may seem problematic. The key here is to understand that the continuation codeexcludec passed into codeexcludefact_cps will be the final block which control is transferred to. That just means that we will need to specify the last block. It’s likely easy to see this if we study composition of CPS functions:highlightcodeexcludehaskellf1 k a = k $ some_computation a\nf2 k a = k $ other_computation a\ncompose k f1 f2 a = (f1 (f2 k) a)\n-- == k $ other_computation $ some_computation aSo the passed continuation codeexcludek is always invoked last.highlightcodeexcludehaskell1: (%1) {\n    %2 = primitive_check(%1, 0)\n    check %2 then invoke 4 (1)\n    invoke 3 (%1, 1)\n}\n\n2: (%1, %2) {\n    %3 = primitive_check(%1, 0)\n    check %3 then invoke 4 (%2)\n    invoke 3 (%1, %2)\n}\n\n3: (%1, %2) {\n    %3 = primitive_mul(%1, %2)\n    %4 = primitive_sub(%1, 1)\n    invoke 2 (%4, %3)\n}\n\n4: (%1) {\n    return %1\n}Above is one example of how this might be represented. It is interesting to study the control flow graph (CFG) of this example.dotimages/dot_g20992.svgSSA & delimited controlOne interesting thought experiment is studying a modified block language where blocks also accept label or continuation arguments:highlightcodeexcludehaskell1: <l> (%1, %2)  {\n    %3 = intrinsic_add(%1, %2)\n    %4 = intrinsic_add(%3, %3)\n    invoke l (%4)\n    return %4\n}For the moment, let’s ignore the optimization requirement that blocks always transfer control away, and never require resumption.Suppose codeexcludel wants to return to the invocation site with a special operator called codeexcluderesumehighlightcodeexcludehaskell1: <l> (%1, %2)  {\n    %3 = intrinsic_add(%1, %2)\n    %4 = intrinsic_add(%3, %3)\n    invoke l (%4)\n    invoke 2 (%4)\n}\n\n2: <> (%1) {\n    %3 = intrinsic_add(%1, %1)\n    return %3\n}where codeexcludel is something like the following:highlightcodeexcludehaskelll: (%1) {\n    %2 = some_computation(%1)\n    resume\n}What has to happen to allow resumption? Well, the computation continuing from the codeexcludeinvoke point needs to be captured by the runtime and resumed when codeexcluderesume occurs:highlightcodeexcludehaskell1: <k> (%1, %2)  {\n    %3 = intrinsic_add(%1, %2)\n    %4 = intrinsic_add(%3, %3)\n    invoke k (%4) -- may resume here.\n\n    -- Captures %4 from the current stack frame.\n    invoke 2 (%4)\n    -- must be captured up to here.\n}\n\n2: <> (%1) {\n    %3 = intrinsic_add(%1, %1)\n    return %3\n}In the CFG, this causes a “kink”:dotimages/dot_g20993.svgBut here, notice that we don’t need to restore the entire stack frame - just the part which is captured by the bounds of the resumption.There is a series of concepts (as far as I can tell, first explored in Lisp) called control operators and delimited control which refer to the same sort of “capturing” mechanism.highlightcodeexcludehaskell(* 2 (reset (+ 1 (shift k (k 5)))))In this code, the operators codeexcludeshift and codeexcludereset set limits on what is captured into the continuation codeexcludek. Here, the continuation codeexclude\\k . (+ 1 k) is captured and then immediately applied.highlightcodeexcludehaskell(* 2 (reset (+ 1 (shift k (k 5)))))\n-- => k == \\x . (+ 1 x)\n-- -> (* 2 (k 5))\n-- -> (* 2 (\\x . (+ 1 x) 5))\n-- -> (* 2 6)\n-- -> 12","id":"notes/cpssa.html","title":"Some thoughts about CPS & SSA"},"notes/gadts.html":{"body":"Generalized algebraic data typesKey to Haskell’s capability of providing the means to construct type-safe embedded domain-specific languages is a concept called generalized algebraic data types (GADTs). I’d like to discuss this briefly–as understanding this concept turns out to be tremendously useful when experimenting with little languages (for research or otherwise).In Haskell, users are allowed to specify data types which are parametrized by type parameters:highlightcodeexcludehaskelldata List a = Nil | Cons a (List a)Now, when I construct an inhabitant of this type, the parameters are inferred automatically:highlightcodeexcludehaskell-- Typed as List Int\nmy_list = Cons 12 (Cons 107 Nil)A generalized algebraic data type can provide a type instantiation of the data type as the return type of a set of constructors. This gives the programmer explicit control over the type interpretation of inhabitants of the algebraic data type.highlightcodeexcludehaskell-- Example from: https://en.wikipedia.org/wiki/Generalized_algebraic_data_type\n\n-- A GADT\n-- where `a` instantiates over the return type of each constructor.\ndata Expr a where\n    EBool  :: Bool     -> Expr Bool\n    EInt   :: Int      -> Expr Int\n    EEqual :: Expr Int -> Expr Int  -> Expr Bool\n\neval :: Expr a -> a\n\neval e = case e of\n    EBool a    -> a\n    EInt a     -> a\n    EEqual a b -> (eval a) == (eval b)\n\nexpr1 = EEqual (EInt 2) (EInt 3)        -- the type of expr1 is Expr Bool\nret = eval expr1                        -- ret is FalseImagine trying to encode this example using parametrized algebraic data types:highlightcodeexcludehaskellExpr a b = EBool a | EInt a | EEqual a bAny instantiation using codeexcludeEEqual _ _ will be typed as codeexcludeExpr Int Int. In this example, this restriction prevents the user from controlling typelevel information about any assumed evaluation semantics (here, codified by codeexcludeeval). If we wanted to write codeexcludeeval on the parametrized type, it becomes much messier, even if we allowed multiparameter type classes.highlightcodeexcludehaskellclass Evaluation a b c where\n    eval :: Expr a b -> c\n\ninstance Evaluation Bool () Bool where\n    eval (EBool a) = a\n\ninstance Evaluation Int () Int where\n    eval (EInt a) = a\n\ninstance Evaluation Int Int Bool where\n    eval (EEqual a b) = (eval a) == (eval b)Representable, but less terse (which is valued in idiomatic modern Haskell). Outside of small calculator languages, things can quickly become quite messy–whereas usage of GADTs affords the programmer some level of control over the type interpretation of terms in their language.An extended tutorial which discusses the same concepts in greater detail is https://en.wikibooks.org/wiki/Haskell/GADTavailable on the Haskell wikibooks.","id":"notes/gadts.html","title":"Generalized algebraic data types"},"notes/inference_calibration.html":{"body":"Rank statistics and inference calibrationRank statistics and inference calibrationInference calibration encapsulates a set of techniques used to quantify distribution approximation error in Bayesian approximate inference. In the context of Markov chain Monte Carlo (MCMC), a distribution approximation algorithm used extensively by statisticians and probabilistic modelers alike, numerous techniques have been developed to facilitate the process of diagnosing problems with inference (and indeed, convergence of MCMC).In this note, I’ll be covering https://cfreer.org/papers/SFAM-goftests.pdfthis paper which develops an exact goodness-of-fit test for high-dimensional discrete distributions. Between this paper, and generalizations of a technique called simulation-based calibration (SBC), there are more than a few techniques in the arsenal for detecting distributional mismatch.Simulation-based calibrationSBC takes advantage of the fact that samples $(z, x)$ from a probabilistic joint $P(z, x)$ for $z$ are also samples from the posterior $P(z | x)$.Given the ability to sample from the joint $P(z, x)$, and an algorithm which approximates $P(z | x)$, for each joint sample $(z, x)$, we can ask the approximation algorithm to approximate $P(z | x)$ from $x$ and then compute a rank statistic with respect to the “ground truth” $z$. Computation of the rank statistic requires that the space $Z$ (where $z \\in Z$) provides a total order relation on elements.Provided that this is true, repeatedly computing $P(z’ \\lt z | x)$ for pairs $(z, x)$ using the approximation algorithm posterior $P’(z | x)$ furnishes an interesting property: $P(z’ \\lt z | x) \\sim \\text{Uniform}(0, 1)$ iff the approximation is close to the true posterior.Stochastic rank statistic","id":"notes/inference_calibration.html","title":"Rank statistics and inference calibration"},"notes/math_is.html":{"body":"The mathematical foundations of importance samplingProbabilistic modeling and inference deals with probability distributions - mathematical objects which are formally equivalent to https://en.wikipedia.org/wiki/Measure_(mathematics)measures.A measure $\\mu$ is a non-negative map from a set $X$ equipped with a https://en.wikipedia.org/wiki/%CE%A3-algebra$\\sigma$-algebra (denoted $\\Sigma$) to $\\mathbb{R}$ which satisfies a few properties (all stated with respect to the underlying $\\sigma$-algebra).itemizeNon-negativity: as already stated, for all elements $E$ of $\\Sigma$, $\\mu(E) \\geq 0$.Null goes to 0: $\\mu(\\phi) = 0$Countable additivity: for all countable collections $\\{E_k\\}_{k=1}^\\infty$ of pairwise disjoint sets in $\\Sigma$, $\\mu(\\cup_{k=1}^\\infty E_k) = \\sum_{k = 1}^\\infty \\mu(E_k)$.The full triple $(X, \\Sigma, \\mu)$ is called a measure space. Probablity measures are measures with an additional property: $\\mu(X) = 1$.Useful to know - but we often work with densities or probability mass functions. How do these objects arise?Probability mass functions arise when considering probability measures over discrete or countably infinite spaces - in this case, we generally are working with the measure directly! E.g. evaluating the query “what is the probability of the random variable taking value this primitive set\" means evaluating the measure on that primitive set (which, by construction, is in the $\\sigma$-algebra).Density functions arise when considering probability measures over $\\mathbb{R}$ or $\\mathbb{R}^N$. To understand how they arise, it is important to understand that there is a “canonical” measure on $\\mathbb{R}^N$ - the https://en.wikipedia.org/wiki/Lebesgue_measureLebesgue measure. Here, I’m implicitly hiding the $\\sigma$-algebra - which requires more discussion. Buying the construction of the Lebesgue measure for the moment, we can construct new measures from an existing reference measure by writing a non-negative function $f: \\mathbb{R}^N \\rightarrow \\mathbb{R}$ and expressing our new measure as the integral of this function using the reference measure over measurable subsets of the triple $(\\mathbb{R}^N, \\Sigma, \\mu)$:\\begin{equation}\\nu(A) = \\int_A f \\ d\\mu\\end{equation}This non-negative function is called a density - these functions are the objects which we are accustomed to working with.","id":"notes/math_is.html","title":"The mathematical foundations of importance sampling"},"notes/smc_samplers.html":{"body":"Notes on sequential Monte CarloThere is a critical (and still highly relevant) paper on sequential Monte Carlo (SMC) samplers which deserves a read from all those interesting in probabilistic modeling and approximate inference.The paper is https://www.stats.ox.ac.uk/~doucet/delmoral_doucet_jasra_sequentialmontecarlosamplersJRSSB.pdfSequential Monte Carlo samplers by Pierre Del Moral, Arnaud Doucet, and Ajay Jasra (DMDJ) - whose contributions include laying the conceptual bricks for modern variants of SMC by carefully discussing a methodology to sample sequentially from a sequence of probability distributions defined on the same space.In this note, I’ll be working through the paper - elaborating where I may, and deferring to other material as required.BackgroundSampling from probability distributions is a commonly desired operation in probabilistic modeling and inference. Especially in the latter case, direct sampling can sometimes be intractable - either because the distribution in question may be defined indirectly through marginalization of an existing density (which is often intractable, although sometimes not) or because https://web.mit.edu/urban_or_book/www/book/chapter7/7.1.3.htmlexisting methods do not apply to densities which lack an analytic CDF.Practitioners have constructed clever approximation techniques to get around these limitations, including Monte Carlo methods - algorithms which internally use randomness to approximately perform optimization, numerical integration, and sampling from target probability distributions.Sequential Monte Carlo (SMC) is one such method with a unique set of computational trade offs compared to other approximate inference techniques. One way of understanding sequential Monte Carlo steps is as incremental importance sampling, where particle populations are evolved and re-weighted at each step - including the effects of conditioning on new observations. Formally, SMC allows us to think about the transition:\\begin{equation}P(\\vec{z}_t | \\vec{x_t}) \\rightarrow P(\\vec{z}_{t + 1} | \\vec{x}_{t + 1})\\end{equation}if I have a particle representation of the LHS, I can perform an SMC update to target the RHS.Comparison to Markov chain Monte CarloAs part of the motivation, (DMDJ) discuss a number of computational comparisons between SMC samplers and Markov chain Monte Carlo (MCMC) samplers.itemizeFor batch observations $y_1, ..., y_n$, MCMC sampling require complete likelihood evaluation at each step. This must include all observations, which may be prohibitively expensive. Instead, it may be computationally beneficial to evaluate likelihoods on partial sets of observations in sequence.For computational reasons, it may be useful to sample sequentially from a series of distributions, beginning with an easy-to-sample distribution $\\pi_1$ and ending with the distribution of interest $\\pi_p$.A simple working exampleOne easy way to understand why this technique is useful is to imagine that a you want to update a distribution in light of new evidence in an online fashion (one new observation every so often).images/dot_g9589.svgGraphical model representation of an unrolled hidden Markov model.Traditionally, SMC algorithms were developed to solve dynamical systems filtering problems. Let’s look at the decomposition of the joint from the model displayed above.\\begin{equation}P(X_0, X_1, X_2, Y_0, Y_1, Y_2) = P(X_0) \\prod_{i = 0} P(X_{i + 1} | X_i)P(Y_i | X_i)\\end{equation}Now, from this joint, we could use SMC to compute approximations to the sequence of distributions $P(X_0 | Y_0)$, $P(X_0, X_1| Y_0, Y_1)$, $P(X_0, X_1, X_2| Y_0, Y_1, Y_2)$ in turn.\\begin{equation}\\begin{aligned}\nT_0^i &\\sim Q(T_0; Y_0), \\hat{w}_0^i = \\frac{P(T_0^i, Y_0)}{Q(T_0^i)}\\\\\nT_t^i &\\sim Q(T_{t - 1}^i; Y_t), \\hat{w}_t^i = \\frac{P(T_t^i,..., T_0^i, Y_t,...,Y_0)}{Q(T_t^i)}\n\\end{aligned}\\end{equation}where $T_t^i$ stands for the i-th sample representation of the latent space at time step $t$. The “importance weight” (from importance sampling) of particle $i$ at time step $t$ is $\\hat{w}_t$ is the pointwise evaluation of the https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem#Radon%E2%80%93Nikodym_derivativeRadon-Nikodym derivative of the measure associated with the model density $P$ and the measure associated with the proposal density $Q$.noteI discuss Radon-Nikodym derivatives in xlinknotes/math_is.htmlxlink../notes/math_is.htmlThe mathematical foundations of importance sampling, but also see the Wikipedia page linked above.From the importance weights, it’s easy to spot the “absolute continuity” condition: $Q$ can’t assign zero probability to a primitive set that $P$ assigns non-zero probability.This presentation is originally from https://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdfDoucet, de Freitas, and Gordon. This presentation should also be praised for treatment of Monte Carlo sampling in general (and in computational comparison with techniques like numerical integration).Sequential importance samplingSequential importance resamplingConditional SMCResample-move SMCIn this section, I’d like to discuss a modification to the SMC framework discussed above which introduces MCMC steps in between SMC update steps. The resultant algorithm class is profoundly interesting. I discuss this class in more depth in xlinknotes/unreasonable_smc.htmlxlink../notes/unreasonable_smc.htmlThe unreasonable effectiveness of rejuvenation move SMC.Relevant worksThere are quite a few relevant works which have contributed to my own presentation here:itemizehttps://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdfDoucet, de Freitas, and Gordon, 2001http://www.mathcs.emory.edu/~whalen/Papers/BNs/MonteCarlo-DBNs.pdfGilks and Berzuini, 2001https://www.stats.ox.ac.uk/~doucet/delmoral_doucet_jasra_sequentialmontecarlosamplersJRSSB.pdfDel Moral, Doucet, and Jasra, 2006https://www.stats.ox.ac.uk/~doucet/doucet_johansen_tutorialPF2011.pdfDoucet and Johansen, 2008","id":"notes/smc_samplers.html","title":"Notes on sequential Monte Carlo"},"notes/unreasonable_smc.html":{"body":"The unreasonable effectiveness of rejuvenation move SMCIn this note, I’ll be covering an approximate inference algorithm design framework which extends SMC with Monte Carlo proposal moves at each step of the algorithm.Previously, smc_samplers.htmlI’ve covered introductory material on sequential Monte Carlo (SMC), where I mentioned some of the benefits of SMC in comparison to techniques like Markov chain Monte Carlo (MCMC).Here, I’ll be discussing https://dl.acm.org/doi/10.1145/3296979.3192399Cusumano-Towner et al, 2018 - a paper which extends SMC to support incremental inference.","id":"notes/unreasonable_smc.html","title":"The unreasonable effectiveness of rejuvenation move SMC"}}